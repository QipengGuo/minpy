{
 "metadata": {
  "name": "",
  "signature": "sha256:7b9068449a24fb3c1207dbc09b849e4a4b239b68d27f03cd9057aca1dc350f85"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#you can get the training data from https://github.com/clab/dynet_tutorial_examples\n",
      "\n",
      "from __future__ import division\n",
      "import minpy.numpy as np\n",
      "from minpy.context import set_context, gpu\n",
      "from minpy.nn import layers\n",
      "from minpy.nn.model import ModelBase\n",
      "from minpy.nn import optim, init\n",
      "from minpy import core\n",
      "import re\n",
      "import random\n",
      "import time\n",
      "\n",
      "class Vocab(object):\n",
      "    def __init__(self, w2i):\n",
      "        self.w2i = dict(w2i)\n",
      "        self.i2w = {i:w for w,i in w2i.iteritems()}\n",
      "\n",
      "    @classmethod\n",
      "    def from_list(cls, words):\n",
      "        w2i = {}\n",
      "        idx = 0\n",
      "        for word in words:\n",
      "            w2i[word] = idx\n",
      "            idx += 1\n",
      "        return Vocab(w2i)\n",
      "\n",
      "    @classmethod\n",
      "    def from_file(cls, vocab_fname):\n",
      "        words = []\n",
      "        with file(vocab_fname) as fh:\n",
      "            for line in fh:\n",
      "                line.strip()\n",
      "                word, count = line.split()\n",
      "                words.append(word)\n",
      "        return Vocab.from_list(words)\n",
      "\n",
      "    def size(self): return len(self.w2i.keys())\n",
      "    \n",
      "def read_oracle(fname, vw, va):\n",
      "    with file(fname) as fh:\n",
      "        for line in fh:\n",
      "            line = line.strip()\n",
      "            ssent, sacts = re.split(r' \\|\\|\\| ', line)\n",
      "            sent = [vw.w2i[x] for x in ssent.split()]\n",
      "            acts = [va.w2i[x] for x in sacts.split()]\n",
      "            sent.reverse()\n",
      "            acts.reverse()\n",
      "            yield (sent, acts)\n",
      "            \n",
      "set_context(gpu(0)) # set the global context with gpu\n",
      "\n",
      "def log_softmax(x):\n",
      "    # x should be (batch, prob)\n",
      "    # y should be (batch, )\n",
      "\n",
      "    x_dev = x - np.max(x, axis=1, keepdims=True) # minpy doesn't support x.max()\n",
      "    sm = x_dev - np.log(np.sum(np.exp(x_dev), axis=1, keepdims=True))\n",
      "    return sm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "acts = ['SHIFT', 'REDUCE_L', 'REDUCE_R']\n",
      "vocab_acts = Vocab.from_list(acts)\n",
      "SHIFT = vocab_acts.w2i['SHIFT']\n",
      "REDUCE_L = vocab_acts.w2i['REDUCE_L']\n",
      "REDUCE_R = vocab_acts.w2i['REDUCE_R']\n",
      "# load training and dev data\n",
      "vocab_words = Vocab.from_file('data/vocab.txt')\n",
      "train = list(read_oracle('data/small-train.unk.txt', vocab_words, vocab_acts))\n",
      "dev = list(read_oracle('data/small-dev.unk.txt', vocab_words, vocab_acts))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "WORD_EMB_DIM = 64\n",
      "ACT_EMB_DIM = 32\n",
      "H_DIM = 64\n",
      "class Stack_RNN(ModelBase):\n",
      "    def __init__(self, batch_size=64, vocab=None, update_rule='adam'):\n",
      "        super(Stack_RNN, self).__init__()\n",
      "        self.vocab = vocab\n",
      "        self.WORD_DIM = vocab.size()\n",
      "        self.batch_size = batch_size\n",
      "        \n",
      "        self.params_config = {}\n",
      "        self.update_rule = getattr(optim, update_rule)\n",
      "        self.optim_config = {}\n",
      "\n",
      "        # fixed a bug of W_comp and b_comp from dynet code\n",
      "        \n",
      "        self.add_param('buffer_head', (self.batch_size, H_DIM))\\\n",
      "            .add_param('tok_emb', (self.WORD_DIM, WORD_EMB_DIM))\\\n",
      "            .add_param('W_s2h_1', (H_DIM, H_DIM))\\\n",
      "            .add_param('W_s2h_2', (H_DIM, H_DIM))\\\n",
      "            .add_param('b_s2h', (H_DIM, ))\\\n",
      "            .add_param('W_act', (H_DIM, 3))\\\n",
      "            .add_param('b_act', (3,))\\\n",
      "            .add_param('W_comp_1', (H_DIM, WORD_EMB_DIM))\\\n",
      "            .add_param('W_comp_2', (H_DIM, WORD_EMB_DIM))\\\n",
      "            .add_param('b_comp', (WORD_EMB_DIM,))\\\n",
      "            .add_param('Wx_buffer_lstm', (WORD_EMB_DIM, 4*H_DIM))\\\n",
      "            .add_param('Wh_buffer_lstm', (H_DIM, 4*H_DIM))\\\n",
      "            .add_param('b_buffer_lstm', (4*H_DIM, ))\\\n",
      "            .add_param('Wx_stack_lstm', (WORD_EMB_DIM, 4*H_DIM))\\\n",
      "            .add_param('Wh_stack_lstm', (H_DIM, 4*H_DIM))\\\n",
      "            .add_param('b_stack_lstm', (4*H_DIM, ))\n",
      "\n",
      "\n",
      "\n",
      "    def init(self, init_rule):\n",
      "        init_rule = getattr(init, init_rule)\n",
      "        for name, config in self.param_configs.items():\n",
      "            self.params[name] = init_rule(config['shape'], {})\n",
      "\n",
      "    def train_step(self, tokens, oracle_actions=None):\n",
      "\n",
      "        def loss_func(*params):\n",
      "            return self.forward(tokens, oracle_actions=oracle_actions)\n",
      "\n",
      "        param_arrays = list(self.params.values())\n",
      "        param_keys = list(self.params.keys())\n",
      "        grad_and_loss_func = core.grad_and_loss(loss_func, argnum=range(len(param_arrays)))\n",
      "        grad_arrays, loss = grad_and_loss_func(*param_arrays)\n",
      "        grads = dict(zip(param_keys, grad_arrays))\n",
      "\n",
      "        #print self.params.keys()[0], grads[self.params.keys()[0]], loss\n",
      "        #for p, g in grads.items():\n",
      "        #    print p\n",
      "        #    print g\n",
      "        grad_norm = 0.0\n",
      "        for p, w in self.params.items():\n",
      "            dw = grads[p]\n",
      "            next_w, next_conf = self.update_rule(w, dw, self.params_config.get(p, None))\n",
      "            self.params[p] = next_w\n",
      "            self.params_config[p] = next_conf\n",
      "            #print dw\n",
      "            if not hasattr(dw, 'shape'):\n",
      "                print p, dw\n",
      "                continue\n",
      "            else:\n",
      "                grad_norm += np.sum(dw**2)\n",
      "\n",
      "        #print 'norm {0:.4f}'.format(grad_norm.asnumpy())\n",
      "        return loss\n",
      "\n",
      "    def test_step(self, tokens, oracle_actions=None):\n",
      "        return self.forward(tokens, oracle_actions)\n",
      "\n",
      "    def _lstm_step(self, x, prev_h, prev_c, Wx, Wh, b):\n",
      "        if prev_h is None:\n",
      "            prev_h = np.zeros((self.batch_size, H_DIM))\n",
      "        if prev_c is None:\n",
      "            prev_c = np.zeros((self.batch_size, H_DIM))\n",
      "        h, c = layers.lstm_step(x, prev_h, prev_c, Wx, Wh, b)\n",
      "        return h, c\n",
      "\n",
      "    def forward(self, tokens, oracle_actions=None):\n",
      "\n",
      "        buf_Wx = self.params['Wx_buffer_lstm']\n",
      "        buf_Wh = self.params['Wh_buffer_lstm']\n",
      "        buf_b = self.params['b_buffer_lstm']\n",
      "        stack_Wx = self.params['Wx_stack_lstm']\n",
      "        stack_Wh = self.params['Wh_stack_lstm']\n",
      "        stack_b = self.params['b_stack_lstm']\n",
      "        \n",
      "        if oracle_actions is not None:\n",
      "            oracle_actions = list(oracle_actions) # aim to support push and pop\n",
      "        buffer_list = []\n",
      "        stack_list = []\n",
      "        \n",
      "        losses = []\n",
      "        for tok in tokens:\n",
      "            tok_emb = self.params['tok_emb'][tok:tok+1]\n",
      "            if len(buffer_list) == 0:\n",
      "                buffer_list.append((self._lstm_step(tok_emb, None, None, buf_Wx, buf_Wh, buf_b), (tok_emb, self.vocab.i2w[tok])))\n",
      "            else:\n",
      "                buffer_list.append((self._lstm_step(tok_emb, buffer_list[-1][0][0], buffer_list[-1][0][1], buf_Wx, buf_Wh, buf_b), (tok_emb, self.vocab.i2w[tok])))\n",
      "\n",
      "        while not (len(stack_list) == 1 and len(buffer_list) == 0):\n",
      "            valid_acts = []\n",
      "            if len(buffer_list) > 0:\n",
      "                valid_acts += [SHIFT]\n",
      "            if len(stack_list) >= 2:\n",
      "                valid_acts += [REDUCE_L, REDUCE_R] \n",
      "            if len(valid_acts)==0:\n",
      "                return 0.0\n",
      "            log_probs = None\n",
      "            action = valid_acts[0] # using SHIFT by default\n",
      "            if len(valid_acts) > 1:\n",
      "                buffer_rep = buffer_list[-1][0][0] if len(buffer_list)>0 else self.params['buffer_head']\n",
      "                stack_rep = stack_list[-1][0][0]\n",
      "                h = np.tanh(np.dot(buffer_rep, self.params['W_s2h_1'])+np.dot(stack_rep, self.params['W_s2h_2'])+self.params['b_s2h'])\n",
      "                #p_t = np.concatenate([buffer_rep, stack_rep], axis=1)\n",
      "                #h = np.tanh(np.dot(p_t, self.params['W_s2h']) + self.params['b_s2h'])\n",
      "                log_probs = log_softmax(np.dot(h, self.params['W_act']) + self.params['b_act'])\n",
      "                for act in [SHIFT, REDUCE_L, REDUCE_R]:\n",
      "                    if act not in valid_acts:\n",
      "                        log_probs[0][act] -= 99999.0\n",
      "                    \n",
      "                if oracle_actions is None:\n",
      "                    action = int(np.argmax(log_probs[0], axis=0)[0])\n",
      "            if oracle_actions is not None:\n",
      "                action = oracle_actions.pop()\n",
      "            if log_probs is not None:\n",
      "                losses.append(log_probs[0][action])\n",
      "\n",
      "            #print len(stack_list), len(buffer_list), action\n",
      "            if action == SHIFT:\n",
      "                tok_emb, tok = buffer_list.pop()[1]\n",
      "                if len(stack_list)==0:\n",
      "                    stack_list.append((self._lstm_step(tok_emb, None, None, stack_Wx, stack_Wh, stack_b), (tok_emb, tok)))\n",
      "                else:\n",
      "                    stack_list.append((self._lstm_step(tok_emb, stack_list[-1][0][0], stack_list[-1][0][1], stack_Wx, stack_Wh, stack_b), (tok_emb, tok)))\n",
      "            else:\n",
      "                right = stack_list.pop()[1]\n",
      "                left = stack_list.pop()[1]\n",
      "                head, modifier = (left, right) if action == REDUCE_R else (right, left)\n",
      "\n",
      "                head_rep, head_tok = head\n",
      "                mod_rep, mod_tok = modifier\n",
      "                composed_rep = np.tanh(np.dot(head_rep, self.params['W_comp_1']) + np.dot(mod_rep, self.params['W_comp_2']) + self.params['b_comp'])\n",
      "                #composed_rep = np.tanh(np.dot(np.concatenate([head_rep, mod_rep], axis=1), self.params['W_comp']) + self.params['b_comp'])\n",
      "                \n",
      "                if len(stack_list)==0:\n",
      "                    stack_list.append((self._lstm_step(composed_rep, None, None, stack_Wx, stack_Wh, stack_b), (composed_rep, tok)))\n",
      "                else:\n",
      "                    stack_list.append((self._lstm_step(composed_rep, stack_list[-1][0][0], stack_list[-1][0][1], stack_Wx, stack_Wh, stack_b), (composed_rep, tok)))\n",
      "                    \n",
      "                if oracle_actions is None:\n",
      "                    print '{0} --> {1}'.format(head_tok, mod_tok)\n",
      "        if oracle_actions is None:\n",
      "            head = stack_list.pop()[1][1]\n",
      "            print 'ROOT --> {0}'.format(head)\n",
      "        total_loss = 0.\n",
      "        for each_loss in losses:\n",
      "            total_loss += each_loss\n",
      "        return -total_loss#total_loss"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = Stack_RNN(batch_size=1, vocab=vocab_words)\n",
      "model.init('xavier')\n",
      "instances_processed = 0\n",
      "validation_losses = []\n",
      "#assert 1==2\n",
      "for epoch in range(1):\n",
      "    random.shuffle(train)\n",
      "    words = 0\n",
      "    total_loss = 0.0\n",
      "    st = time.time()\n",
      "    for (s,a) in train:\n",
      "        # periodically report validation loss\n",
      "        e = instances_processed / len(train)\n",
      "        if instances_processed % 1000 == 0:\n",
      "            dev_words = 0\n",
      "            dev_loss = 0.0\n",
      "            st = time.time()\n",
      "            cnt = 0\n",
      "            for (ds, da) in dev:\n",
      "                loss = model.test_step(ds, da)\n",
      "                dev_words += len(ds)\n",
      "                if loss is not None:\n",
      "                    dev_loss += loss\n",
      "                #print cnt, \n",
      "                cnt+=1\n",
      "            #print ' '\n",
      "            print('[validation] time {} epoch {}: per-word loss: {}'.format(time.time()-st, e, dev_loss / dev_words))\n",
      "            validation_losses.append(dev_loss)\n",
      "            st = time.time()\n",
      "\n",
      "        # report training loss\n",
      "\n",
      "        if instances_processed % 100 == 0 and words > 0:\n",
      "            print('time {}: epoch {}: per-word loss: {}'.format(time.time()-st, e, total_loss / words))\n",
      "            words = 0\n",
      "            total_loss = 0.0\n",
      "            st = time.time()\n",
      "        if e>0.2:\n",
      "            break\n",
      "        # here we do training\n",
      "        loss = model.train_step(s, a) # returns None for 1-word sentencs (it's clear how to parse them)\n",
      "        words += len(s)\n",
      "        instances_processed += 1\n",
      "        if loss is not None:\n",
      "            total_loss += float(loss)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[validation] time 123.456494093 epoch 0.0: per-word loss: 2.11330201985\n",
        "time 103.088572979: epoch 0.01: per-word loss: 1.4528251943"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time 97.3443000317: epoch 0.02: per-word loss: 1.2006842051"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time 104.218065023: epoch 0.03: per-word loss: 1.04773040323"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time 97.3555641174: epoch 0.04: per-word loss: 0.91827727099"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time 110.941392899: epoch 0.05: per-word loss: 0.894304969604"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wh_buffer_lstm"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0\n",
        "W_comp_2 0.0\n",
        "W_comp_1 0.0\n",
        "b_comp 0.0\n",
        "b_buffer_lstm 0.0\n",
        "Wx_buffer_lstm 0.0\n",
        "tok_emb"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0\n",
        "b_s2h 0.0\n",
        "Wh_buffer_lstm 0.0\n",
        "b_act 0.0\n",
        "W_s2h_1 0.0\n",
        "Wh_stack_lstm 0.0\n",
        "W_s2h_2 0.0\n",
        "W_comp_2 0.0\n",
        "buffer_head 0.0\n",
        "W_comp_1 0.0\n",
        "b_comp 0.0\n",
        "b_buffer_lstm 0.0\n",
        "b_stack_lstm 0.0\n",
        "W_act 0.0\n",
        "Wx_stack_lstm 0.0\n",
        "Wx_buffer_lstm 0.0\n",
        "time 94.8959817886: epoch 0.06: per-word loss: 0.782395024872\n",
        "Wh_buffer_lstm"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0\n",
        "W_comp_2 0.0\n",
        "W_comp_1 0.0\n",
        "b_comp 0.0\n",
        "b_buffer_lstm 0.0\n",
        "Wx_buffer_lstm 0.0\n",
        "time 109.598351955: epoch 0.07: per-word loss: 0.749221507094"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tok_emb"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0\n",
        "b_s2h 0.0\n",
        "Wh_buffer_lstm 0.0\n",
        "b_act 0.0\n",
        "W_s2h_1 0.0\n",
        "Wh_stack_lstm 0.0\n",
        "W_s2h_2 0.0\n",
        "W_comp_2 0.0\n",
        "buffer_head 0.0\n",
        "W_comp_1 0.0\n",
        "b_comp 0.0\n",
        "b_buffer_lstm 0.0\n",
        "b_stack_lstm 0.0\n",
        "W_act 0.0\n",
        "Wx_stack_lstm 0.0\n",
        "Wx_buffer_lstm 0.0\n",
        "time 101.373834848: epoch 0.08: per-word loss: 0.680411092434"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time 95.1358239651: epoch 0.09: per-word loss: 0.68143913082"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[validation] time 108.709712029 epoch 0.1: per-word loss: 0.663471816718"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time 2.86102294922e-06: epoch 0.1: per-word loss: 0.563871891249\n",
        "time 107.729501963: epoch 0.11: per-word loss: 0.60616555975"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time 106.21041894: epoch 0.12: per-word loss: 0.575962388596"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time 97.6490750313: epoch 0.13: per-word loss: 0.540574414454"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time 102.963398933: epoch 0.14: per-word loss: 0.565105997041"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wh_buffer_lstm"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0\n",
        "W_comp_2 0.0\n",
        "W_comp_1 0.0\n",
        "b_comp 0.0\n",
        "b_buffer_lstm 0.0\n",
        "Wx_buffer_lstm 0.0\n",
        "time 95.7812349796: epoch 0.15: per-word loss: 0.53578390087"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tok_emb"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0\n",
        "b_s2h 0.0\n",
        "Wh_buffer_lstm 0.0\n",
        "b_act 0.0\n",
        "W_s2h_1 0.0\n",
        "Wh_stack_lstm 0.0\n",
        "W_s2h_2 0.0\n",
        "W_comp_2 0.0\n",
        "buffer_head 0.0\n",
        "W_comp_1 0.0\n",
        "b_comp 0.0\n",
        "b_buffer_lstm 0.0\n",
        "b_stack_lstm 0.0\n",
        "W_act 0.0\n",
        "Wx_stack_lstm 0.0\n",
        "Wx_buffer_lstm 0.0\n",
        "time 98.4253730774: epoch 0.16: per-word loss: 0.50803818647"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time 100.773450136: epoch 0.17: per-word loss: 0.493155689443"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time 105.156779051: epoch 0.18: per-word loss: 0.507457974089"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tok_emb"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0\n",
        "b_s2h 0.0\n",
        "Wh_buffer_lstm 0.0\n",
        "b_act 0.0\n",
        "W_s2h_1 0.0\n",
        "Wh_stack_lstm 0.0\n",
        "W_s2h_2 0.0\n",
        "W_comp_2 0.0\n",
        "buffer_head 0.0\n",
        "W_comp_1 0.0\n",
        "b_comp 0.0\n",
        "b_buffer_lstm 0.0\n",
        "b_stack_lstm 0.0\n",
        "W_act 0.0\n",
        "Wx_stack_lstm 0.0\n",
        "Wx_buffer_lstm 0.0\n",
        "time 100.051234961: epoch 0.19: per-word loss: 0.462437459672"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[validation] time 118.60631299 epoch 0.2: per-word loss: 0.501328141515"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "time 3.81469726562e-06: epoch 0.2: per-word loss: 0.514507027144\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(train)*0.01"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "100.0"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = 'Parsing in Austin is fun .'\n",
      "UNK = vocab_words.w2i['<unk>']\n",
      "toks = [vocab_words.w2i[x] if x in vocab_words.w2i else UNK for x in s.split()]\n",
      "toks.reverse()\n",
      "model.test_step(toks)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "in --> Austin\n",
        "<unk> --> Austin\n",
        "fun --> is\n",
        "fun --> Austin\n",
        "fun --> ."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ROOT --> .\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "0.65141934009443503"
       ]
      }
     ],
     "prompt_number": 6
    }
   ],
   "metadata": {}
  }
 ]
}