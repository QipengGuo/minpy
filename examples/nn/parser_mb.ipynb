{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#you can get the training data from https://github.com/clab/dynet_tutorial_examples\n",
    "\n",
    "from __future__ import division\n",
    "import minpy.numpy as np\n",
    "from minpy.context import set_context, gpu\n",
    "from minpy.nn.model_builder import *\n",
    "from minpy.nn.modules import *\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self, w2i):\n",
    "        self.w2i = dict(w2i)\n",
    "        self.i2w = {i:w for w,i in w2i.iteritems()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_list(cls, words):\n",
    "        w2i = {}\n",
    "        idx = 0\n",
    "        for word in words:\n",
    "            w2i[word] = idx\n",
    "            idx += 1\n",
    "        return Vocab(w2i)\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, vocab_fname):\n",
    "        words = []\n",
    "        with file(vocab_fname) as fh:\n",
    "            for line in fh:\n",
    "                line.strip()\n",
    "                word, count = line.split()\n",
    "                words.append(word)\n",
    "        return Vocab.from_list(words)\n",
    "\n",
    "    def size(self): return len(self.w2i.keys())\n",
    "    \n",
    "def read_oracle(fname, vw, va):\n",
    "    with file(fname) as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            ssent, sacts = re.split(r' \\|\\|\\| ', line)\n",
    "            sent = [vw.w2i[x] for x in ssent.split()]\n",
    "            acts = [va.w2i[x] for x in sacts.split()]\n",
    "            sent.reverse()\n",
    "            acts.reverse()\n",
    "            yield (sent, acts)\n",
    "            \n",
    "set_context(gpu(0)) # set the global context with gpu\n",
    "\n",
    "def log_softmax(x):\n",
    "    # x should be (batch, prob)\n",
    "    # y should be (batch, )\n",
    "\n",
    "    x_dev = x - np.max(x, axis=1, keepdims=True) # minpy doesn't support x.max()\n",
    "    sm = x_dev - np.log(np.sum(np.exp(x_dev), axis=1, keepdims=True))\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acts = ['SHIFT', 'REDUCE_L', 'REDUCE_R']\n",
    "vocab_acts = Vocab.from_list(acts)\n",
    "SHIFT = vocab_acts.w2i['SHIFT']\n",
    "REDUCE_L = vocab_acts.w2i['REDUCE_L']\n",
    "REDUCE_R = vocab_acts.w2i['REDUCE_R']\n",
    "# load training and dev data\n",
    "vocab_words = Vocab.from_file('data/vocab.txt')\n",
    "train = list(read_oracle('data/small-train.unk.txt', vocab_words, vocab_acts))\n",
    "dev = list(read_oracle('data/small-dev.unk.txt', vocab_words, vocab_acts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORD_EMB_DIM = 64\n",
    "ACT_EMB_DIM = 32\n",
    "H_DIM = 64\n",
    "class Stack_RNN(ModelBase):\n",
    "    def __init__(self, batch_size=64, vocab=None):\n",
    "        super(Stack_RNN, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.WORD_DIM = vocab.size()\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "\n",
    "        self._stack_lstm = LSTM(H_DIM, 'tanh')\n",
    "        self._comp_linear = FullyConnected(H_DIM, WORD_EMB_DIM)\n",
    "        self._buffer_lstm = LSTM(H_DIM, 'tanh')\n",
    "        self._act_linear = FullyConnected(H_DIM, 3)\n",
    "        self._s2h_linaer = FullyConnected(H_DIM, H_DIM)\n",
    "        self._tok_emb = Embedding(sefl.WORD_DIM, WORD_EMB_DIM)\n",
    "        \n",
    "        self._buffer_head = Variable((self.batch_size, H_DIM))\n",
    "        \n",
    "\n",
    "    def forward(self, tokens, oracle_actions=None):\n",
    "        \n",
    "        if oracle_actions is not None:\n",
    "            oracle_actions = list(oracle_actions) # aim to support push and pop\n",
    "        buffer_list = []\n",
    "        stack_list = []\n",
    "        \n",
    "        losses = []\n",
    "        for tok in tokens:\n",
    "            tok_emb = self._tok_emb(tok) #shape error, should be [tok] or tok.reshape((1, -1))\n",
    "            if len(buffer_list) == 0:\n",
    "                buffer_list.append((self._buffer_lstm(tok_emb, None, None), (tok_emb, self.vocab.i2w[tok])))\n",
    "            else:\n",
    "                buffer_list.append((self._buffer_lstm(tok_emb, buffer_list[-1][0][0], buffer_list[-1][0][1]), (tok_emb, self.vocab.i2w[tok])))\n",
    "\n",
    "        while not (len(stack_list) == 1 and len(buffer_list) == 0):\n",
    "            valid_acts = []\n",
    "            if len(buffer_list) > 0:\n",
    "                valid_acts += [SHIFT]\n",
    "            if len(stack_list) >= 2:\n",
    "                valid_acts += [REDUCE_L, REDUCE_R] \n",
    "            if len(valid_acts)==0:\n",
    "                return 0.0\n",
    "            log_probs = None\n",
    "            action = valid_acts[0] # using SHIFT by default\n",
    "            if len(valid_acts) > 1:\n",
    "                buffer_rep = buffer_list[-1][0][0] if len(buffer_list)>0 else self._buffer_head()\n",
    "                stack_rep = stack_list[-1][0][0]\n",
    "                p_t = np.concatenate([buffer_rep, stack_rep], axis=1)\n",
    "                h = np.tanh(self._s2h_linear(p_t))\n",
    "                log_probs = log_softmax(self._act_linear(h))\n",
    "                for act in [SHIFT, REDUCE_L, REDUCE_R]:\n",
    "                    if act not in valid_acts:\n",
    "                        log_probs[act] -= 99999.0  # shape error, log_probs[0][act] -= 99999.0\n",
    "                    \n",
    "                if oracle_actions is None:\n",
    "                    action = int(np.argmax(log_probs[0], axis=0)[0]) #shape error, int(np.argmax(log_probs[0], axis=0)[0])\n",
    "            if oracle_actions is not None:\n",
    "                action = oracle_actions.pop()\n",
    "            if log_probs is not None:\n",
    "                losses.append(log_probs[action]) #shape error, losses.append(log_probs[0][action]) \n",
    "\n",
    "            #print len(stack_list), len(buffer_list), action\n",
    "            if action == SHIFT:\n",
    "                tok_emb, tok = buffer_list.pop()[1]\n",
    "                if len(stack_list)==0:\n",
    "                    stack_list.append((self._stack_lstm(tok_emb, None, None), (tok_emb, tok)))\n",
    "                else:\n",
    "                    stack_list.append((self._stack_lstm(tok_emb, stack_list[-1][0][0], stack_list[-1][0][1]), (tok_emb, tok)))\n",
    "            else:\n",
    "                right = stack_list.pop()[1]\n",
    "                left = stack_list.pop()[1]\n",
    "                head, modifier = (left, right) if action == REDUCE_R else (right, left)\n",
    "\n",
    "                head_rep, head_tok = head\n",
    "                mod_rep, mod_tok = modifier\n",
    "                composed_rep = np.tanh(self._comp_linear(np.concatenate([head_rep, mod_rep], axis=1)))\n",
    "                \n",
    "                if len(stack_list)==0:\n",
    "                    stack_list.append((self._stack_lstm(composed_rep, None, None), (composed_rep, tok)))\n",
    "                else:\n",
    "                    stack_list.append((self._stack_lstm(composed_rep, stack_list[-1][0][0], stack_list[-1][0][1]), (composed_rep, tok)))\n",
    "                    \n",
    "                if oracle_actions is None:\n",
    "                    print '{0} --> {1}'.format(head_tok, mod_tok)\n",
    "        if oracle_actions is None:\n",
    "            head = stack_list.pop()[1][1]\n",
    "            print 'ROOT --> {0}'.format(head)\n",
    "        #minpy error, doesn't support np.sum(list)\n",
    "        #total_loss = 0.\n",
    "        #for each_loss in losses:\n",
    "        #    total_loss += each_loss\n",
    "        return - np.sum(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Stack_RNN(batch_size=1, vocab=vocab_words)\n",
    "updater = Updater(model, update_rule='rmsprop', learning_rate=0.002)\n",
    "instances_processed = 0\n",
    "validation_losses = []\n",
    "#assert 1==2\n",
    "for epoch in range(1):\n",
    "    random.shuffle(train)\n",
    "    words = 0\n",
    "    total_loss = 0.0\n",
    "    st = time.time()\n",
    "    for (s,a) in train:\n",
    "        # periodically report validation loss\n",
    "        e = instances_processed / len(train)\n",
    "        if instances_processed % 1000 == 0:\n",
    "            dev_words = 0\n",
    "            dev_loss = 0.0\n",
    "            st = time.time()\n",
    "            cnt = 0\n",
    "            for (ds, da) in dev:\n",
    "                loss = model.forward(ds, da)\n",
    "                dev_words += len(ds)\n",
    "                if loss is not None:\n",
    "                    dev_loss += loss\n",
    "                #print cnt, \n",
    "                cnt+=1\n",
    "            #print ' '\n",
    "            print('[validation] time {} epoch {}: per-word loss: {}'.format(time.time()-st, e, dev_loss / dev_words))\n",
    "            validation_losses.append(dev_loss)\n",
    "            st = time.time()\n",
    "\n",
    "        # report training loss\n",
    "\n",
    "        if instances_processed % 100 == 0 and words > 0:\n",
    "            print('time {}: epoch {}: per-word loss: {}'.format(time.time()-st, e, total_loss / words))\n",
    "            words = 0\n",
    "            total_loss = 0.0\n",
    "            st = time.time()\n",
    "        if e>0.2:\n",
    "            break\n",
    "        # here we do training\n",
    "        grad_dict, loss = model.grad_and_loss(s, a) # returns None for 1-word sentencs (it's clear how to parse them)\n",
    "        updater(grad_dict) \n",
    "        words += len(s)\n",
    "        instances_processed += 1\n",
    "        if loss is not None:\n",
    "            total_loss += float(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
